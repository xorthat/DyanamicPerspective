\documentclass[12pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{subcaption}

\cvprfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\setcounter{page}{1}
\begin{document}

\title{3D Perspective Effects on a Smart Phone}

\author{Jai Prakash\\
Carnegie Mellon University\\
Master of Science in Computer Vision\\
{\tt\small jprakash@andrew.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Jennifer Lake\\
Carnegie Mellon University\\
Master of Science in Computer Vision\\
{\tt\small jelake@andrew.cmu.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this project, we aim to create a 3D perspective transformation on a smart phone that will give the illusion of negative parallax.  The user should feel that there is depth to the smart phone screen, rather than just a flat screen (citation).   This is achieved by finding the three-dimensional vector between the user’s face and the center of the phone.  This was achieved by tracking the user’s face using the front-facing camera and calculating the smart phone’s orientation using the Internal Measurement Unit (IMU) and using Kalman filtering to smooth out the measurements.  Finally, these two pieces of information are combined to find the desired vector to create the perspective illusion.
\end{abstract}

\section{Introduction}
\subsection{Motivation}
As mobile phones have evolved over the last twenty years, many of the major milestones have been in display improvements. Mobile phones have gone from very small, simple displays to larger and more colorful displays.  At the forefront of this evolution, there is a growing trend of three-dimensional displays.  The primary motivation driving this trend in 3D perspective displays is to give the user more a more immersive user experience.

In 2013, Apple introduced the parallax effect on their line of iPhones, which allows for a 3D-like feeling, just short of a full 3D perspective effect \cite{BusinessInsider}.  In 2015, the Amazon Fire Phone introduced a full 3D perspective effect, which was branded as "Dynamic Perspective" \cite{DigitalTrends}.  It is this type of effect that we aim to create in this project however, unlike the Amazon Fire Phone, we will be implementing this effect on simpler, standard smart phone hardware
\subsection{Background}
    
\subsection{Related Work}
\subsubsection{Amazon Fire Phone}
     -Jenna
\subsubsection{Virtual Window}
     -Jenna
     
\begin{figure}
\begin{subfigure}{0.2\textwidth}
\includegraphics[scale=0.16]{images/AR_now}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\includegraphics[scale=0.22]{images/AR_now1}
\end{subfigure}
\caption{AR in present smartphones}
\label{fig:arnow}
\end{figure}

\begin{figure}
\includegraphics[scale=0.16]{images/mata}
\caption{Direct AR from Meta-glasses}
\label{fig:directar}
\end{figure}


\begin{figure}
\begin{subfigure}{0.22\textwidth}
\includegraphics[scale=0.3]{images/transparenttablet}
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
\includegraphics[scale=0.4]{images/transparenttablet1}
\end{subfigure}
\caption{Digital transparency using Kinect on Android tablet}
\label{fig:digitaltransparency}
\end{figure}

     
\subsubsection{Digital Transparency}
One more motivation for this project come from the augmented reality (AR) in present smartphones. The AR is smartphones can majorly be classified into two types
\begin{itemize}
\item \textbf{Indirect AR} Figure \ref{fig:arnow} shows augmented reality applications in current smartphones. This gives the user a video see-through experience, as the view is from the camera's perspective.
\item \textbf{Direct AR} These are the systems that give a perspective from user's point of view and hence give more immersive experience than indirect AR system. Holo-lens and meta-glasses (Figure \ref{fig:directar}) are examples of direct AR systems. 
\end{itemize}

The current direct AR systems are generally bulky and requires user to wear special glasses to use them. However, a direct AR system can be achieved in smartphones and tablets by using digital transparency \cite{jai}. Digital transparency system is shown in Figure \ref{fig:digitaltransparency}. This system tracks the eyes of the user and finds the angle which the tablet subtends at user's eye and crops the rear camera preview to the same angle to create a virtually transparent interface. This system gives a method to achieve direct AR on smartphones/tablets and hence gives a more immersive \textbf{glass-see-through} experience than conventional \textbf{video-see-through} experience.

This method uses a kinect sensor to track the accurate 3D position of the user with respect to tablet. The kinect sensor makes the system bulkier and difficult to use in smartphones. So, in this project we are addressing another method to achieve digital transparency using just the front facing camera and inertial sensors. A complete hands-free digital transparency system is out of scope of this project. The main concentration is on creating a dynamic perspective system which responds to the user's position with respect to the tablet.

\section{Method}
\begin{figure}
\includegraphics[scale=0.21]{images/block}
\caption{Block diagram of dynamic perspective system}
\label{fig:blockdia}
\end{figure}

Creating a dynamic perspective system involves finding the viewing angle of the user and rendering the graphical object depending on position of the user. This might involve three scenario
\begin{itemize}
\item Tablet is static and user moves
\item Tablet moves and user is static
\item Both tablet and the user can move
\end{itemize}

To find the motion and orientation of the phone, on-board inertial sensors can be deployed to find the orientation of the phone. A study on sensor fusion has been made to find the orientation of the phone.

The block diagram of the system is shown in the figure  \ref{fig:blockdia}. Solid arrow shows the algorithms implemented for this project. The dotted arrow parts are yet to be figured out to build a full fledge digital transparency system. However, the main focus is only on creating the dynamic perspective.

To create a dynamic perspective, we need to first track the position of user with respect to the tablet. In this project we assume that the distance of the eye from tablet is fixed (approximately 45-50 cm). This assumptions fails to address the scale factor in dynamic perspective. Hence, we don't get zoom-in and zoom-out effect as we move forward and backward. Since the distance from the tablet is assumed to be constant, we use the viewing angle to render the user interface (UI) in 3D. The following sections describe the exact methods used in each step.

\subsection{3D Geometry of Problem}
    -Jenna. describe the vectors involved
\subsection{Face Detection}
    -Jenna, I'm assuming this is Viola-Jones

\subsection{Kalman Filter}
The face detection using the cascade classifier is subject to jitter due to noise in the scene. The face detection sometimes fails because of various reasons like change is lighting conditions, change in expression of the face etc. This detection might cause jitter in the rendering. In order to avoid the jitter, we need to smooth the face detection and handle the no-detection case. The Kalman filter \cite{kalman}, can handle statistical uncertainties and other accuracies and produce results that are more accurate than a single measurement.

\subsection{Finding viewing angle of user}
The viewing angle is one of the critical measurements for creating a dynamic perspective system. After applying the Kalman filter to smoothen the face detection, we need to find the viewing angle with respect to the camera. Since all the coordinates are in image frame of reference, the angle can be found by using the following equations. 
\begin{equation}
\theta_x = tan^{-1} \left( \frac{x_f - \frac{W}{2}}{f_x} \right)
\label{eqn:thetax}
\end{equation}
\begin{equation}
\theta_y = tan^{-1} \left( \frac{y_f - \frac{H}{2}}{f_y} \right)
\label{eqn:thetay}
\end{equation}

In equation \ref{eqn:thetax} and \ref{eqn:thetay}, $(x_f, y_f)$ is the location of the centroid of the user's face. Image size is $W \times H$. $f_x$ and $f_y$ are the focal length of the camera in pixels.

\begin{figure}
\includegraphics[scale=0.5]{images/view_angle}
\caption{The viewing angle of the user with respect to front camera on tablet}
\label{fig:viewangle}
\end{figure}

Figure \ref{fig:viewangle} shows the viewing angle of the user. The view angle is the angle subtended by the user's position with the principal axis of the camera. The focal length of the camera is found by using camera calibration in opencv \cite{calibration}.


\subsection{Classifier}
    -Jai, not sure if this is needed.
\subsection{Inertial Measurement Unit}
    -Jenna
\subsection{Extended Kalman Filter}
    -Jenna
\section{Experiments}

\subsection{Camera calibration}
The camera calibration matrix is returned as
\begin{equation}
K = \begin{bmatrix}
f_x & \alpha_x & c_x\\
0 & f_y &  c_y\\
0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
145 & 0 & 930\\
 0 &145 & 601\\
0 & 0 & 1
\end{bmatrix}
\end{equation}

The value of the focal length is in terms of the pixels. The resolution of the camea is 1920x1280, and the principal axis seems to be almost at the center of the image.

\subsection{Multi-View Geometry Experiments}
    -Jai, make subsubsections as needed
\subsection{IMU Experiments}
    -Jenna, make subsubsections as needed
\section{Conclusions}
    -Jai
{\small{
\begin{thebibliography}{11}

\bibitem{BusinessInsider}
Yarow, Jay. "Here's Why Apple Made That Motion-Effect For The Background Of The New IPhone Software." Business Insider. Business Insider, Inc, 25 Sept. 2013. Web. 11 Dec. 2015.

\bibitem{DigitalTrends}
Pelegrin, Williams. "Why Isn’t the Fire Phone Truly 3D? Amazon’s ‘Dynamic Perspective’ Tech Explained." \textit{Digital Trends}. Digital Trends, 18 June 2014. Web. 11 Dec. 2015.

\bibitem{jai}
J. Prakash and A. Vijayvargiya. A method for obtaining digital transparency in Electronic Devices, Indian Patent 4506/CHE/2014

\bibitem{calibration}
\url{http://docs.opencv.org/3.0-beta/doc/tutorials/calib3d/camera_calibration/camera_calibration.html}

\bibitem{kalman}
 Kalman, R. E. "A New Approach to Linear Filtering and Prediction Problems". \textit{Journal of Basic Engineering, 1960}

\end{thebibliography}

\end{document}

